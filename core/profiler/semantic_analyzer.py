import json
import logging
import os
from pathlib import Path
from typing import Dict, Any

# å¼•å…¥æˆ‘ä»¬ä¹‹å‰å†™å¥½çš„æ¨¡å—
# æ³¨æ„ï¼šå¦‚æœè¿è¡Œæ—¶æç¤º ModuleNotFoundErrorï¼Œè¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œï¼Œæˆ–è®¾ç½® PYTHONPATH
try:
    from core.ingestion.loader_factory import LoaderFactory
    from core.llm.ollama_client import LocalLlamaClient
    from core.profiler.basic_stats import get_dataset_fingerprint
except ImportError:
    # å…¼å®¹ç›´æ¥è¿è¡Œæ­¤è„šæœ¬æ—¶çš„è·¯å¾„é—®é¢˜
    import sys

    sys.path.append(str(Path(__file__).resolve().parent.parent.parent))
    from core.ingestion.loader_factory import LoaderFactory
    from core.llm.ollama_client import LocalLlamaClient
    from core.profiler.basic_stats import get_dataset_fingerprint

logger = logging.getLogger(__name__)


class SemanticAnalyzer:
    def __init__(self, llm_client: LocalLlamaClient):
        self.llm = llm_client

    def _build_prompt(self, filename: str, fingerprint: Dict[str, Any]) -> str:
        """
        æ„å»ºå‘é€ç»™ Llama 3.1 çš„ Prompt (ç»è¿‡å¹»è§‰æŠ‘åˆ¶ä¼˜åŒ–)ã€‚
        """
        # 1. æ„å»ºåˆ—çš„è¯¦ç»†æè¿°æ¸…å•
        columns_summary = []
        column_names_list = []  # è®°å½•åŸå§‹åˆ—åï¼Œç”¨äºåç»­æ ¡éªŒ

        for col, info in fingerprint["column_stats"].items():
            column_names_list.append(col)
            desc = f"Column: '{col}' | Type: {info['dtype']} | Samples: {info['samples']}"
            if "min" in info:
                desc += f" | Range: {info['min']:.2f} to {info['max']:.2f}"
            if "geom_type" in info:
                desc += f" | Geometry Type: {info['geom_type']}"
            columns_summary.append(desc)

        columns_text = "\n".join(columns_summary)

        # 2. å®šä¹‰æ›´åŠ ç²¾å‡†çš„è¯­ä¹‰æ ‡ç­¾é›†
        allowed_tags = """
        - ST_TIME: Timestamp, date, datetime (e.g., pickup_time)
        - ST_LOC_ID: Location IDs, Zone IDs (e.g., PULocationID)
        - ST_GEO: Geometry column (WKT, geometry objects)
        - ST_LAT: Latitude coordinates
        - ST_LON: Longitude coordinates
        - BIZ_METRIC: Numerical measures (distance, passenger_count, speed)
        - BIZ_PRICE: Monetary values (fare, total_amount)
        - BIZ_CAT: Categorical data (VendorID, payment_type, Zone Names)
        - ID_KEY: Primary keys or foreign keys (row_id)
        - OTHER: Anything else
        """

        # 3. å¼ºçº¦æŸ Prompt
        prompt = f"""
        You are a Spatial Data Expert. I need you to analyze the schema of a dataset.

        === DATASET METADATA ===
        File Name: "{filename}"
        Rows: {fingerprint['rows']}
        CRS: {fingerprint.get('crs', 'N/A')}

        === ACTUAL COLUMNS (Use ONLY these names as keys) ===
        {columns_text}

        === INSTRUCTIONS ===
        1. Analyze the 'ACTUAL COLUMNS' list above.
        2. Map EACH column name to exactly one semantic tag from the list below:
        {allowed_tags}

        3. Determine the 'dataset_type':
           - TRAJECTORY: Has time + space (points/lines) + metrics.
           - GEO_ZONE: Has polygons/multipolygons (reference map).
           - LOOKUP_TABLE: Has IDs and Names but no geometry/coordinates.

        4. Strict JSON Output Rules:
           - The keys in "semantic_tags" MUST be the exact column names from the input.
           - DO NOT invent new columns.
           - DO NOT use the tags as keys.

        === RESPONSE FORMAT (JSON ONLY) ===
        {{
            "dataset_type": "TRAJECTORY/GEO_ZONE/LOOKUP_TABLE",
            "description": "Short summary",
            "semantic_tags": {{
                "{column_names_list[0]}": "TAG",
                "{column_names_list[1]}": "TAG"
            }}
        }}
        """
        return prompt

    def analyze(self, file_path: str) -> Dict[str, Any]:
        """
        ä¸»å…¥å£ï¼šåˆ†ææ–‡ä»¶å¹¶è¿”å›å¢å¼ºåçš„å…ƒæ•°æ®
        """
        logger.info(f"Starting semantic analysis for: {file_path}")

        try:
            loader = LoaderFactory.get_loader(file_path)

            # Action 1: è·å–çœŸå®çš„è¡Œæ•° (å…¨é‡æ‰«æ/å…ƒæ•°æ®è¯»å–)
            real_row_count = loader.count_rows(file_path)

            # Action 2: è·å–æ ·æœ¬æ•°æ® (åªå–å‰ 10 è¡Œç»™ AI çœ‹)
            df_preview = loader.peek(file_path, n=10)

        except Exception as e:
            logger.error(f"Loader failed: {e}")
            return {"error": f"Failed to load file: {str(e)}"}

        # 3. è®¡ç®—åŸºç¡€ç»Ÿè®¡æŒ‡çº¹
        try:
            fingerprint = get_dataset_fingerprint(df_preview)

            # [å…³é”®ä¿®æ­£]ï¼šç”¨çœŸå®è¡Œæ•°è¦†ç›–æ ·æœ¬è¡Œæ•°ï¼
            fingerprint['rows'] = real_row_count

        except Exception as e:
            logger.error(f"Fingerprinting failed: {e}")
            return {"error": f"Failed to generate stats: {str(e)}"}

        # 4. æ„å»º Prompt (åç»­é€»è¾‘ä¸å˜...)
        filename = Path(file_path).name
        prompt = self._build_prompt(filename, fingerprint)

        # 5. è°ƒç”¨ LLM è·å–è¯­ä¹‰æ ‡ç­¾
        try:
            print(f"   >>> Sending metadata of [{filename}] to Llama 3.1...")
            ai_result = self.llm.query_json(
                prompt=prompt,
                system_prompt="You are a data analysis assistant that outputs only valid JSON."
            )
        except Exception as e:
            logger.error(f"LLM inference failed: {e}")
            ai_result = {
                "dataset_type": "UNKNOWN",
                "description": "AI analysis failed.",
                "semantic_tags": {}
            }

        # 6. åˆå¹¶ç»“æœ
        final_summary = {
            "file_info": {
                "path": str(file_path),
                "name": filename
            },
            "basic_stats": fingerprint,
            "semantic_analysis": ai_result
        }

        return final_summary


# --- å®æˆ˜æµ‹è¯•éƒ¨åˆ† ---
if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—çº§åˆ«ä»¥ä¾¿è§‚å¯Ÿè¿‡ç¨‹
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

    # 1. è‡ªåŠ¨å®šä½ data ç›®å½•
    # å‡è®¾ç»“æ„æ˜¯ NL-STV/core/profiler/semantic_analyzer.py
    # data ç›®å½•åœ¨ NL-STV/data
    current_dir = Path(__file__).resolve().parent
    project_root = current_dir.parent.parent
    data_dir = project_root / "data"

    print(f"ğŸ” Searching for data files in: {data_dir}")

    # 2. æ‰«æç›®å½•ä¸‹å¸¸è§æ ¼å¼çš„æ–‡ä»¶
    target_extensions = ['*.csv', '*.parquet', '*.shp']
    found_files = []
    for ext in target_extensions:
        found_files.extend(list(data_dir.glob(ext)))

    if not found_files:
        print("âŒ No files found in data directory! Please check your path.")
        exit()

    # 3. åˆå§‹åŒ– AI å®¢æˆ·ç«¯
    print("ğŸ”Œ Connecting to Local Ollama...")
    try:
        client = LocalLlamaClient(model_name="llama3.1:latest")
        analyzer = SemanticAnalyzer(client)
    except Exception as e:
        print(f"âŒ Failed to init Ollama: {e}")
        exit()

    # 4. éå†æ–‡ä»¶è¿›è¡Œåˆ†æ
    print(f"âœ… Found {len(found_files)} files. Starting Batch Analysis...\n")

    for file_path in found_files:
        print(f"--------------------------------------------------")
        print(f"ğŸ“‚ Processing: {file_path.name}")

        # æ‰§è¡Œåˆ†æ
        result = analyzer.analyze(str(file_path))

        # æ£€æŸ¥æ˜¯å¦å‡ºé”™
        if "error" in result:
            print(f"âŒ Error: {result['error']}")
            continue

        # æå–å…³é”®ä¿¡æ¯è¿›è¡Œå±•ç¤º
        ai_output = result.get("semantic_analysis", {})
        dataset_type = ai_output.get("dataset_type", "UNKNOWN")
        desc = ai_output.get("description", "No description")
        tags = ai_output.get("semantic_tags", {})

        print(f"ğŸ¤– AI Assessment:")
        print(f"   - Type: \033[92m{dataset_type}\033[0m")  # ç»¿è‰²é«˜äº®
        print(f"   - Summary: {desc}")
        print(f"   - Column Mapping:")
        for col, role in tags.items():
            print(f"     * {col:<25} -> {role}")

        print(f"\nâœ… Analysis Complete for {file_path.name}")

    print("\n--------------------------------------------------")
    print("ğŸ‰ All files processed.")