import streamlit as st
import pandas as pd
import geopandas as gpd
import os
import sys
from pathlib import Path
import logging

# --- ç¯å¢ƒè®¾ç½® ---
# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥ core æ¨¡å—
current_dir = Path(__file__).resolve().parent
sys.path.append(str(current_dir))

from core.ingestion.loader_factory import LoaderFactory
from core.llm.ollama_client import LocalLlamaClient
from core.profiler.semantic_analyzer import SemanticAnalyzer

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="NL-STV Platform",
    page_icon="ğŸ—ºï¸",
    layout="wide"
)


# --- å·¥å…·å‡½æ•° ---
def save_uploaded_file(uploaded_file, save_dir="data_sandbox"):
    """å°†ä¸Šä¼ çš„å†…å­˜æ–‡ä»¶ä¿å­˜åˆ°ç£ç›˜ï¼Œä»¥ä¾¿ Loader è¯»å–"""
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    file_path = os.path.join(save_dir, uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return file_path


@st.cache_resource
def get_analyzer():
    """åˆå§‹åŒ– AI åˆ†æå™¨ (å•ä¾‹æ¨¡å¼)"""
    try:
        client = LocalLlamaClient(model_name="llama3.1:latest")
        return SemanticAnalyzer(client)
    except Exception as e:
        st.error(f"æ— æ³•è¿æ¥åˆ° Ollama: {e}")
        return None


# --- ä¸»ç•Œé¢é€»è¾‘ ---

def main():
    st.title("ğŸ—ºï¸ NL-STV: AI é©±åŠ¨çš„æ—¶ç©ºæ•°æ®åˆ†æå¹³å°")
    st.markdown("---")

    # 1. ä¾§è¾¹æ ï¼šæ§åˆ¶ä¸çŠ¶æ€
    with st.sidebar:
        st.header("1. æ•°æ®æ¥å…¥")
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ æ•°æ®æ–‡ä»¶",
            type=["csv", "parquet", "xlsx", "zip"],  # zip ç”¨äº shapefile
            help="æ”¯æŒ CSV, Parquet, Excelã€‚Shapefile è¯·å‹ç¼©ä¸º zip ä¸Šä¼ ã€‚"
        )

        st.info("ğŸ’¡ å½“å‰ä½¿ç”¨æ¨¡å‹: Llama 3.1 (Local)")

    # 2. ä¸»åŒºåŸŸï¼šåˆ†æç»“æœ
    if uploaded_file is not None:
        # ä¿å­˜æ–‡ä»¶
        file_path = save_uploaded_file(uploaded_file)

        col1, col2 = st.columns([1, 2])

        with col1:
            st.success(f"æ–‡ä»¶å·²ä¸Šä¼ : {uploaded_file.name}")
            st.caption(f"å¤§å°: {uploaded_file.size / 1024:.1f} KB")

        # è§¦å‘åˆ†æ
        analyzer = get_analyzer()
        if analyzer:
            with st.spinner("ğŸ¤– AI æ­£åœ¨é˜…è¯»æ•°æ®å¹¶æå–è¯­ä¹‰..."):
                # è°ƒç”¨æˆ‘ä»¬åœ¨åç«¯å†™çš„æ ¸å¿ƒé€»è¾‘
                analysis_result = analyzer.analyze(file_path)

            if "error" in analysis_result:
                st.error(analysis_result["error"])
            else:
                render_analysis_report(analysis_result)
    else:
        render_landing_page()


def render_landing_page():
    st.markdown("""
    ### æ¬¢è¿ä½¿ç”¨è‡ªç„¶è¯­è¨€æ—¶ç©ºå¯è§†åŒ–å™¨

    è¿™æ˜¯ä¸€ä¸ª**é›¶ä»£ç **æ•°æ®åˆ†æå·¥å…·ã€‚æ‚¨åªéœ€è¦ä¸Šä¼ æ–‡ä»¶ï¼ŒAI å°†ä¼šè‡ªåŠ¨ï¼š
    1.  **è¯†åˆ«æ•°æ®ç±»å‹** (è½¨è¿¹ã€åŒºåŸŸã€ODæµå‘)
    2.  **ç†è§£å­—æ®µå«ä¹‰** (å“ªä¸ªæ˜¯æ—¶é—´ï¼Œå“ªä¸ªæ˜¯ç»çº¬åº¦)
    3.  *(å³å°†ä¸Šçº¿)* **ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨**

    ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ æ‚¨çš„ `csv` æˆ– `parquet` æ–‡ä»¶å¼€å§‹ã€‚
    """)


def render_analysis_report(result):
    """æ¸²æŸ“åˆ†ææŠ¥å‘Š"""
    ai_data = result.get("semantic_analysis", {})
    stats = result.get("basic_stats", {})

    # --- é¡¶éƒ¨ï¼šAI æ‘˜è¦ ---
    st.header("2. AI æ•°æ®æ‘˜è¦")

    # ä½¿ç”¨ Metric å±•ç¤ºå…³é”®æŒ‡æ ‡
    m1, m2, m3 = st.columns(3)
    m1.metric("æ•°æ®è¡Œæ•°", f"{stats['rows']:,}")
    m2.metric("å­—æ®µæ•°", stats['cols'])

    # åŠ¨æ€é¢œè‰²æ ‡ç­¾
    d_type = ai_data.get('dataset_type', 'UNKNOWN')
    color = "green" if d_type == "TRAJECTORY" else "blue" if d_type == "GEO_ZONE" else "orange"
    m3.markdown(f"**æ•°æ®ç±»å‹**: :{color}[{d_type}]")

    st.info(f"ğŸ“ **AI è§£è¯»**: {ai_data.get('description', 'æ— æè¿°')}")

    # --- ä¸­éƒ¨ï¼šå­—æ®µè¯­ä¹‰æ˜ å°„ ---
    st.subheader("3. è¯­ä¹‰æ˜ å°„è¡¨")
    st.caption("AI å·²è‡ªåŠ¨è¯†åˆ«ä»¥ä¸‹åˆ—çš„ä¸šåŠ¡å«ä¹‰ï¼š")

    # æ„é€ ä¸€ä¸ªå±•ç¤ºç”¨çš„ DataFrame
    tags = ai_data.get("semantic_tags", {})
    col_stats = stats.get("column_stats", {})

    table_data = []
    for col, role in tags.items():
        # è·å–è¯¥åˆ—çš„ç¤ºä¾‹æ•°æ®
        meta = col_stats.get(col, {})
        samples = str(meta.get("samples", []))
        dtype = meta.get("dtype", "unknown")

        table_data.append({
            "åˆ—å (Column)": col,
            "æ•°æ®ç±»å‹": dtype,
            "AI è¯­ä¹‰æ ‡ç­¾": role,
            "ç¤ºä¾‹å€¼": samples
        })

    st.dataframe(pd.DataFrame(table_data), use_container_width=True)

    # --- åº•éƒ¨ï¼šæ•°æ®é¢„è§ˆ ---
    with st.expander("æŸ¥çœ‹åŸå§‹æ•°æ®é¢„è§ˆ (Top 5 Rows)", expanded=False):
        # é‡æ–°å¿«é€Ÿè¯»å–ä¸€ä¸‹ç”¨äºå±•ç¤º (åˆ©ç”¨ LoaderFactory)
        loader = LoaderFactory.get_loader(result['file_info']['path'])
        df_preview = loader.peek(result['file_info']['path'], n=5)
        st.dataframe(df_preview)


if __name__ == "__main__":
    main()